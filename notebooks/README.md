# Directory For Notebooks For All Experiments
#
[Plagiarism_with_Bert_Trainer_codebert_antlr_lower_4.ipynb](./Plagiarism_with_Bert_Trainer_codebert_antlr_lower_4.ipynb) - model training with using codebert, and data prepartion with ANTLR and whitespace elimination.<br>
[Plagiarism_with_Trainer_Classifiers.ipynb](./Plagiarism_with_Trainer_Classifiers.ipynb) - Latest version of code to run the BERT model in Google Colab environment. Reads data, Sets up model and run training and prediction on the test data. <br>
[Plagiarism_with_Chinese_Bert.ipynb](./Plagiarism_with_Chinese_Bert.ipynb) - Running Chinese version of Codebert trained on masked whole words <br> 
[Plagiarism_with_CodeBert.ipynb](./Plagiarism_with_CodeBert.ipynb) - Model training and data preparation using Codebert base from microsfot<br>
[Plagiarism_with_BigBird.ipynb](./Plagiarism_with_BigBird.ipynb)  - Model training using BigBird to look at the effect of having larger token size <br>
[Plagiarism_with_LongFormer.ipynb](./Plagiarism_with_LongFormer.ipynb) - Model training using Longformer 4K to see the effect of significantly larger size.<br>
[Plagiarism_with_Sentence_Transformer.ipynb](./Plagiarism_with_Sentence_Transformer.ipynb) - Using Sentence Transformers and averaging embeddings over the entire code file, then doing cosine similarities between the average embeddings - done with BERT, CodeBERT, LongFormer and T5Large<br>
[Model_Error_Calculation.ipynb](./Model_Error_Calculation.ipynb) - Colab notebooks to generate the examples of TP/TN/FN/TN and precision, recall and aucpr for an already trained model.



